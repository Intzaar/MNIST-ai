{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "\n",
        "# Linear Layer Class\n",
        "class Linear:\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        \"\"\"\n",
        "        Initializes a fully connected layer with weights and biases.\n",
        "        \"\"\"\n",
        "        self.weights = np.random.randn(input_dim, output_dim) * 0.01\n",
        "        self.biases = np.zeros((1, output_dim))\n",
        "        self.input = None\n",
        "        self.output = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Performs the forward pass through the layer.\n",
        "        \"\"\"\n",
        "        self.input = x\n",
        "        self.output = np.dot(x, self.weights) + self.biases\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, d_out):\n",
        "        \"\"\"\n",
        "        Performs the backward pass, updating the weights and biases.\n",
        "        \"\"\"\n",
        "        self.d_weights = np.dot(self.input.T, d_out)\n",
        "        self.d_biases = np.sum(d_out, axis=0, keepdims=True)\n",
        "        d_input = np.dot(d_out, self.weights.T)\n",
        "\n",
        "        return d_input\n",
        "\n",
        "# ReLU Activation Class\n",
        "class ReLU:\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Applies the ReLU activation function.\n",
        "        \"\"\"\n",
        "        self.input = x\n",
        "        return np.maximum(0, x)\n",
        "\n",
        "    def backward(self, d_out):\n",
        "        \"\"\"\n",
        "        Computes the gradient of the ReLU function.\n",
        "        \"\"\"\n",
        "        return d_out * (self.input > 0)\n",
        "\n",
        "# Sigmoid Activation Class\n",
        "class Sigmoid:\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Applies the Sigmoid activation function.\n",
        "        \"\"\"\n",
        "        self.input = x\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def backward(self, d_out):\n",
        "        \"\"\"\n",
        "        Computes the gradient of the Sigmoid function.\n",
        "        \"\"\"\n",
        "        sigmoid = self.forward(self.input)\n",
        "        return d_out * sigmoid * (1 - sigmoid)\n",
        "\n",
        "# Tanh Activation Class\n",
        "class Tanh:\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Applies the Tanh activation function.\n",
        "        \"\"\"\n",
        "        self.input = x\n",
        "        return np.tanh(x)\n",
        "\n",
        "    def backward(self, d_out):\n",
        "        \"\"\"\n",
        "        Computes the gradient of the Tanh function.\n",
        "        \"\"\"\n",
        "        return d_out * (1 - np.tanh(self.input) ** 2)\n",
        "\n",
        "# Softmax Activation Class\n",
        "class Softmax:\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Applies the Softmax activation function.\n",
        "        \"\"\"\n",
        "        exp_values = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "        self.output = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, d_out):\n",
        "        \"\"\"\n",
        "        Placeholder for backward pass, typically used with Cross-Entropy Loss.\n",
        "        \"\"\"\n",
        "        return d_out  # Simplified for use with Cross-Entropy Loss\n",
        "\n",
        "# Cross-Entropy Loss Class\n",
        "class CrossEntropyLoss:\n",
        "    def forward(self, y_pred, y_true):\n",
        "        \"\"\"\n",
        "        Computes the forward pass of the cross-entropy loss.\n",
        "        \"\"\"\n",
        "        samples = len(y_pred)\n",
        "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
        "        correct_confidences = y_pred_clipped[range(samples), y_true]\n",
        "        loss = -np.log(correct_confidences)\n",
        "        return np.mean(loss)\n",
        "\n",
        "    def backward(self, y_pred, y_true):\n",
        "        \"\"\"\n",
        "        Computes the gradient of the cross-entropy loss.\n",
        "        \"\"\"\n",
        "        samples = len(y_pred)\n",
        "        y_pred[range(samples), y_true] -= 1\n",
        "        return y_pred / samples\n",
        "\n",
        "# Mean Squared Error (MSE) Loss Class\n",
        "class MSELoss:\n",
        "    def forward(self, y_pred, y_true):\n",
        "        \"\"\"\n",
        "        Computes the forward pass of the MSE loss.\n",
        "        \"\"\"\n",
        "        return np.mean((y_pred - y_true) ** 2)\n",
        "\n",
        "    def backward(self, y_pred, y_true):\n",
        "        \"\"\"\n",
        "        Computes the gradient of the MSE loss.\n",
        "        \"\"\"\n",
        "        return 2 * (y_pred - y_true) / y_true.size\n",
        "\n",
        "# SGD Optimizer Class\n",
        "class SGD:\n",
        "    def __init__(self, learning_rate=0.01):\n",
        "        \"\"\"\n",
        "        Initializes the stochastic gradient descent optimizer.\n",
        "        \"\"\"\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    def step(self, layers):\n",
        "        \"\"\"\n",
        "        Updates the weights and biases of each layer.\n",
        "        \"\"\"\n",
        "        for layer in layers:\n",
        "            if hasattr(layer, 'weights'):\n",
        "                layer.weights -= self.learning_rate * layer.d_weights\n",
        "                layer.biases -= self.learning_rate * layer.d_biases\n",
        "\n",
        "# Model Class\n",
        "class Model:\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initializes the model with a list of layers.\n",
        "        \"\"\"\n",
        "        self.layers = []\n",
        "        self.loss = None\n",
        "        self.optimizer = None\n",
        "\n",
        "    def add_layer(self, layer):\n",
        "        \"\"\"\n",
        "        Adds a layer to the model.\n",
        "        \"\"\"\n",
        "        self.layers.append(layer)\n",
        "\n",
        "    def compile(self, loss, optimizer):\n",
        "        \"\"\"\n",
        "        Compiles the model with a loss function and optimizer.\n",
        "        \"\"\"\n",
        "        self.loss = loss\n",
        "        self.optimizer = optimizer\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Performs the forward pass through all layers.\n",
        "        \"\"\"\n",
        "        for layer in self.layers:\n",
        "            x = layer.forward(x)\n",
        "        return x\n",
        "\n",
        "    def backward(self, d_out):\n",
        "        \"\"\"\n",
        "        Performs the backward pass through all layers.\n",
        "        \"\"\"\n",
        "        for layer in reversed(self.layers):\n",
        "            d_out = layer.backward(d_out)\n",
        "\n",
        "    def train(self, x_train, y_train, epochs=20, batch_size=64):\n",
        "        \"\"\"\n",
        "        Trains the model on the given training data.\n",
        "        \"\"\"\n",
        "        for epoch in range(epochs):\n",
        "            for i in range(0, len(x_train), batch_size):\n",
        "                x_batch = x_train[i:i + batch_size]\n",
        "                y_batch = y_train[i:i + batch_size]\n",
        "\n",
        "                y_pred = self.forward(x_batch)\n",
        "                loss = self.loss.forward(y_pred, y_batch)\n",
        "\n",
        "                d_out = self.loss.backward(y_pred, y_batch)\n",
        "                self.backward(d_out)\n",
        "                self.optimizer.step(self.layers)\n",
        "\n",
        "            print(f'Epoch {epoch + 1}, Loss: {loss}')\n",
        "\n",
        "    def predict(self, x):\n",
        "        \"\"\"\n",
        "        Predicts output using the trained model.\n",
        "        \"\"\"\n",
        "        return self.forward(x)\n",
        "\n",
        "    def evaluate(self, x_test, y_test):\n",
        "        \"\"\"\n",
        "        Evaluates the model on the test data.\n",
        "        \"\"\"\n",
        "        y_pred = self.forward(x_test)\n",
        "        loss = self.loss.forward(y_pred, y_test)\n",
        "        accuracy = np.mean(np.argmax(y_pred, axis=1) == y_test)\n",
        "        return loss, accuracy\n",
        "\n",
        "    def save(self, filepath):\n",
        "        \"\"\"\n",
        "        Saves the model weights to a file.\n",
        "        \"\"\"\n",
        "        model_params = [layer.__dict__ for layer in self.layers]\n",
        "        np.save(filepath, model_params)\n",
        "\n",
        "    def load(self, filepath):\n",
        "        \"\"\"\n",
        "        Loads the model weights from a file.\n",
        "        \"\"\"\n",
        "        model_params = np.load(filepath, allow_pickle=True)\n",
        "        for layer, params in zip(self.layers, model_params):\n",
        "            layer.__dict__.update(params)\n"
      ],
      "metadata": {
        "id": "PrkmVpPO3SKR"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train.reshape(x_train.shape[0], -1) / 255.0\n",
        "x_test = x_test.reshape(x_test.shape[0], -1) / 255.0\n",
        "\n",
        "# Define the model\n",
        "model = Model()\n",
        "model.add_layer(Linear(784, 128))\n",
        "model.add_layer(ReLU())\n",
        "model.add_layer(Linear(128, 10))\n",
        "model.add_layer(Softmax())\n",
        "\n",
        "# Compile the model\n",
        "loss = CrossEntropyLoss()\n",
        "optimizer = SGD(learning_rate=0.01)\n",
        "model.compile(loss, optimizer)\n",
        "\n",
        "# Train the model\n",
        "model.train(x_train, y_train, epochs=20, batch_size=64)\n",
        "\n",
        "# Save the model\n",
        "model.save('mnist_model.npy')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "473TdpjnRIQW",
        "outputId": "96466f10-2570-4602-87b4-428bc82abc93"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.9612261202205985\n",
            "Epoch 2, Loss: 0.38328471116218316\n",
            "Epoch 3, Loss: 0.25497216401449396\n",
            "Epoch 4, Loss: 0.20014453996888873\n",
            "Epoch 5, Loss: 0.16996188909791965\n",
            "Epoch 6, Loss: 0.15011477161080464\n",
            "Epoch 7, Loss: 0.13603497095030595\n",
            "Epoch 8, Loss: 0.1251693485829672\n",
            "Epoch 9, Loss: 0.1169721432562974\n",
            "Epoch 10, Loss: 0.10908072997560189\n",
            "Epoch 11, Loss: 0.10153310872373333\n",
            "Epoch 12, Loss: 0.09482178891877731\n",
            "Epoch 13, Loss: 0.08927320163778878\n",
            "Epoch 14, Loss: 0.08439665179697202\n",
            "Epoch 15, Loss: 0.08019829885025592\n",
            "Epoch 16, Loss: 0.07628635914533811\n",
            "Epoch 17, Loss: 0.07250012880212206\n",
            "Epoch 18, Loss: 0.06928022849297849\n",
            "Epoch 19, Loss: 0.06613621374648163\n",
            "Epoch 20, Loss: 0.06334478948827633\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
        "print(f'Test Loss: {test_loss}, Test Accuracy: {test_accuracy}')\n",
        "\n",
        "# Load the model (for demonstration purposes)\n",
        "model.load('mnist_model.npy')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2LuQmgbJTiDQ",
        "outputId": "29c2933b-0b62-4edd-cf71-a81d62e96f2e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.19872000453163513, Test Accuracy: 0.941\n"
          ]
        }
      ]
    }
  ]
}